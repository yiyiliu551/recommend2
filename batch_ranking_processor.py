#!/usr/bin/env python3 """ batchamountSortProcess - combineshelluse ProcessindividualBatch UserSort """ import argparse import redis import json import time import logging from typing import List, Dict import sys import os class BatchRankingProcessor: """batchamountSortProcess""" def __init__(self, redis_host: str, redis_port: int, redis_db: int = 0): self.redis_client = redis.Redis(host=redis_host, port=redis_port, db=redis_db) self.batch_size = 100 #Process100individualUser def load_user_history(self, user_id: int) -> List[int]: """fromRedisLoadUserHistoryBehavior""" key = f"user:history:{user_id}" history = self.redis_client.lrange(key, -50, -1) #most50individualBehavior return [int(h) for h in history] if history else self._generate_mock_history(user_id) def _generate_mock_history(self, user_id: int) -> List[int]: """Generatemock/simulateHistorynumber""" import random random.seed(user_id) #ensureUserHistory return [random.randint(1, 100000) for _ in range(random.randint(10, 50))] def get_recall_candidates(self, user_id: int) -> List[int]: """GetRecallCandidates""" # fromRecallserviceGetCandidates（mock/simulate） import random random.seed(user_id * 2) return random.sample(range(1, 100001), 1000) def deepfm_ranking(self, user_id: int, candidates: List[int]) -> List[Dict]: """DeepFMCoarse ranking""" import random random.seed(user_id * 3) # mock/simulateDeepFMminute scored_items = [] for i, item_id in enumerate(candidates): score = 1.0 - i * 0.0001 + random.uniform(-0.01, 0.01) scored_items.append({ 'item_id': item_id, 'score': max(0.1, score), #ensurepositiveminute 'position': i + 1 }) # minutenumberSort，returnTop 200 scored_items.sort(key=lambda x: x['score'], reverse=True) return scored_items[:200] def export_to_redis(self, user_id: int, rankings: List[Dict]) -> bool: """ExportresulttoRedis""" try: key = f"rec:v2:user:{user_id}" pipe = self.redis_client.pipeline() # Deleteoldnumber pipe.delete(key) # addnew Sortresult zadd_data = {} for rank in rankings: member = f"{rank['item_id']}:{rank['position']}" zadd_data[member] = rank['score'] if zadd_data: pipe.zadd(key, zadd_data) pipe.expire(key, 86400 * 2) #2daythrough pipe.execute() return True except Exception as e: logging.error(f"ExportUser {user_id} resultFailure: {e}") return False def process_single_user(self, user_id: int) -> bool: """ProcessindividualUser Sort""" try: # 1. GetRecallCandidates candidates = self.get_recall_candidates(user_id) # 2. DeepFMCoarse ranking rankings = self.deepfm_ranking(user_id, candidates) # 3. ExporttoRedis success = self.export_to_redis(user_id, rankings) if not success: logging.error(f"User {user_id} ProcessFailure") return False return True except Exception as e: logging.error(f"ProcessUser {user_id} hoursendError: {e}") return False def process_batch(self, user_ids: List[int]) -> Dict: """ProcessbatchUser""" start_time = time.time() success_count = 0 failed_users = [] for user_id in user_ids: if self.process_single_user(user_id): success_count += 1 else: failed_users.append(user_id) end_time = time.time() return { 'total_users': len(user_ids), 'success_count': success_count, 'failed_count': len(failed_users), 'failed_users': failed_users, 'duration': end_time - start_time, 'qps': len(user_ids) / (end_time - start_time) if end_time > start_time else 0 } def setup_logging(log_file: str = None): """Setlog""" log_format = '%(asctime)s - %(levelname)s - %(message)s' if log_file: logging.basicConfig( level=logging.INFO, format=log_format, handlers=[ logging.FileHandler(log_file), logging.StreamHandler(sys.stdout) ] ) else: logging.basicConfig(level=logging.INFO, format=log_format) def main(): parser = argparse.ArgumentParser(description='batchamountSortProcess') parser.add_argument('--user-file', required=True, help='UserIDFilePath') parser.add_argument('--batch-id', required=True, help='BatchID') parser.add_argument('--redis-host', default='localhost', help='Redismain') parser.add_argument('--redis-port', type=int, default=6379, help='Redisport') parser.add_argument('--top-k', type=int, default=200, help='returnTop-KindividualProduct') parser.add_argument('--log-file', help='logFilePath') args = parser.parse_args() # Setlog setup_logging(args.log_file) logging.info(f"Start/BeginProcessBatch {args.batch_id}") logging.info(f"UserFile: {args.user_file}") logging.info(f"Redis: {args.redis_host}:{args.redis_port}") # ReadUserList try: with open(args.user_file, 'r') as f: user_ids = [int(line.strip()) for line in f if line.strip().isdigit()] except Exception as e: logging.error(f"ReadUserFileFailure: {e}") sys.exit(1) logging.info(f"ProcessUsernumber: {len(user_ids)}") # InitializeProcess try: processor = BatchRankingProcessor(args.redis_host, args.redis_port) except Exception as e: logging.error(f"InitializeProcessFailure: {e}") sys.exit(1) # ProcessBatch result = processor.process_batch(user_ids) # Outputresult logging.info(f"Batch {args.batch_id} ProcessComplete:") logging.info(f"totalUsernumber: {result['total_users']}") logging.info(f"Success: {result['success_count']}") logging.info(f"Failure: {result['failed_count']}") logging.info(f"hour: {result['duration']:.2f}second") logging.info(f" QPS: {result['qps']:.2f}") if result['failed_users']: logging.warning(f"failureuser: {result['failed_users'][:10]}...") #front10individual # ifFailureratethroughhigh，exitoutfor1 failure_rate = result['failed_count'] / result['total_users'] if result['total_users'] > 0 else 0 if failure_rate > 0.05: #Failureratethrough5% logging.error(f"Failureratethroughhigh: {failure_rate:.2%}") sys.exit(1) logging.info(f"Batch {args.batch_id} ProcessSuccess") if __name__ == '__main__': main()