""" BERT4RecModelimplementation - fromStart/Begin author: Yang Liu description: BERT4Recimplementation，containsandpoint : BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer (CIKM 2019) """ import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from typing import Dict, List, Tuple, Optional import random import math class MultiHeadAttention(nn.Module): """ multipleheadAttention mechanism - BERT4Rec centergroup isindividualModel close！ """ def __init__(self, hidden_size: int, num_attention_heads: int, dropout_prob: float = 0.1): super().__init__() # ensurehidden_sizeheadnumber assert hidden_size % num_attention_heads == 0 self.num_attention_heads = num_attention_heads self.attention_head_size = hidden_size // num_attention_heads self.all_head_size = hidden_size # Q, K, V linechangeswitchlayer self.query = nn.Linear(hidden_size, self.all_head_size) self.key = nn.Linear(hidden_size, self.all_head_size) self.value = nn.Linear(hidden_size, self.all_head_size) self.dropout = nn.Dropout(dropout_prob) print(f"Multi-head attentionInitialize: {num_attention_heads}head, headdegree{self.attention_head_size}") def transpose_for_scores(self, x): """ willInputreshapeforMulti-head attention style [batch_size, seq_len, hidden_size] -> [batch_size, num_heads, seq_len, head_size] """ new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None): """ Forward propagation - noteCompute/Calculate center """ batch_size, seq_len = hidden_states.size()[:2] # 1. Compute/Calculate Q, K, V query_layer = self.transpose_for_scores(self.query(hidden_states)) key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # 2. Compute/Calculatenoteminutenumber: Q * K^T / sqrt(d_k) attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) # 3. attention mask (blockpaddingwait) if attention_mask is not None: attention_scores = attention_scores + attention_mask # 4. SoftmaxtonoteWeights attention_probs = F.softmax(attention_scores, dim=-1) attention_probs = self.dropout(attention_probs) # 5. addand: Attention * V context_layer = torch.matmul(attention_probs, value_layer) # 6. newreshapereturnbeginstyle context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) return context_layer, attention_probs class FeedForward(nn.Module): """ frontNeural network - Transformer Block partminute """ def __init__(self, hidden_size: int, intermediate_size: int, dropout_prob: float = 0.1): super().__init__() self.dense_1 = nn.Linear(hidden_size, intermediate_size) self.intermediate_act_fn = nn.GELU() #BERTuseGELUactivation self.dense_2 = nn.Linear(intermediate_size, hidden_size) self.dropout = nn.Dropout(dropout_prob) print(f"frontnetwork: {hidden_size} -> {intermediate_size} -> {hidden_size}") def forward(self, hidden_states): """ Forward propagation: Linear -> GELU -> Linear """ hidden_states = self.dense_1(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) hidden_states = self.dense_2(hidden_states) hidden_states = self.dropout(hidden_states) return hidden_states class TransformerBlock(nn.Module): """ Transformer Block - BERT4Rec centerBuildblock = Multi-Head Attention + Feed Forward + Residual + LayerNorm """ def __init__(self, hidden_size: int, num_attention_heads: int, intermediate_size: int, dropout_prob: float = 0.1): super().__init__() # Multi-head attentionlayer self.attention = MultiHeadAttention(hidden_size, num_attention_heads, dropout_prob) # frontnetworklayer self.feed_forward = FeedForward(hidden_size, intermediate_size, dropout_prob) # Layer Normalization self.attention_layernorm = nn.LayerNorm(hidden_size, eps=1e-12) self.output_layernorm = nn.LayerNorm(hidden_size, eps=1e-12) # Dropout self.dropout = nn.Dropout(dropout_prob) print(f"Transformer BlockBuildComplete") def forward(self, hidden_states, attention_mask=None): """ Forward propagation - Transformer Block """ # 1. Multi-Head Attention + Residual Connection + LayerNorm attention_output, attention_probs = self.attention(hidden_states, attention_mask) attention_output = self.dropout(attention_output) attention_output = self.attention_layernorm(attention_output + hidden_states) # 2. Feed Forward + Residual Connection + LayerNorm feed_forward_output = self.feed_forward(attention_output) layer_output = self.output_layernorm(feed_forward_output + attention_output) return layer_output, attention_probs class BERT4RecEmbeddings(nn.Module): """ BERT4Rec Embeddinglayer = Item Embedding + Position Embedding """ def __init__(self, vocab_size: int, hidden_size: int, max_seq_length: int, dropout_prob: float = 0.1): super().__init__() # ItemEmbedding (ProductID -> Vector) self.item_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0) # Embedding ( -> Vector) self.position_embeddings = nn.Embedding(max_seq_length, hidden_size) # LayerNormandDropout self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12) self.dropout = nn.Dropout(dropout_prob) print(f"🔤 EmbeddinglayerInitialize:") print(f"Producttablelargesmall: {vocab_size}") print(f"Embeddingdegree: {hidden_size}") print(f"mostlargedegree: {max_seq_length}") def forward(self, input_ids): """ Forward propagation - Generate Embeddingtable """ batch_size, seq_length = input_ids.size() # GenerateID: [0, 1, 2, ..., seq_length-1] position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand(batch_size, -1) # GetEmbedding item_embeddings = self.item_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # addtomostendEmbedding embeddings = item_embeddings + position_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings class BERT4RecEncoder(nn.Module): """ BERT4RecEncoding - multiplelayerTransformer Blocks """ def __init__(self, hidden_size: int, num_hidden_layers: int, num_attention_heads: int, intermediate_size: int, dropout_prob: float = 0.1): super().__init__() # multiplelayerTransformer Blocks self.layer = nn.ModuleList([ TransformerBlock(hidden_size, num_attention_heads, intermediate_size, dropout_prob) for _ in range(num_hidden_layers) ]) print(f"🏗️ EncodingInitialize: {num_hidden_layers}layerTransformer") def forward(self, hidden_states, attention_mask=None, output_all_encoded_layers=True): """ Forward propagation - layerEncoding """ all_encoder_layers = [] all_attention_probs = [] for layer_module in self.layer: hidden_states, attention_probs = layer_module(hidden_states, attention_mask) if output_all_encoded_layers: all_encoder_layers.append(hidden_states) all_attention_probs.append(attention_probs) if not output_all_encoded_layers: all_encoder_layers.append(hidden_states) all_attention_probs.append(attention_probs) return all_encoder_layers, all_attention_probs class BERT4RecModel(nn.Module): """ BERT4RecmainModel recommendModelimplementation """ def __init__(self, config: Dict): super().__init__() # ConfigurationParameters self.vocab_size = config['vocab_size'] self.hidden_size = config['hidden_size'] self.num_hidden_layers = config['num_hidden_layers'] self.num_attention_heads = config['num_attention_heads'] self.intermediate_size = config['intermediate_size'] self.max_seq_length = config['max_seq_length'] self.dropout_prob = config.get('dropout_prob', 0.1) print("🚀 BERT4RecModelInitializeStart/Begin") print(f"ConfigurationParameters: {config}") # Embeddinglayer self.embeddings = BERT4RecEmbeddings( self.vocab_size, self.hidden_size, self.max_seq_length, self.dropout_prob ) # Encoding self.encoder = BERT4RecEncoder( self.hidden_size, self.num_hidden_layers, self.num_attention_heads, self.intermediate_size, self.dropout_prob ) # Predictionhead (MLM) self.cls = BERT4RecPredictionHead(self.hidden_size, self.vocab_size) # InitializeWeights self.apply(self._init_weights) print("✅ BERT4RecModelInitializeComplete") def _init_weights(self, module): """WeightsInitialize - important """ if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=0.02) elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() def forward(self, input_ids, attention_mask=None): """ Forward propagation - main Inference """ # 1. Embeddinglayer embedding_output = self.embeddings(input_ids) # 2. Encoding encoded_layers, attention_probs = self.encoder( embedding_output, attention_mask, output_all_encoded_layers=False ) # 3. GetList (mostbacklayer) sequence_output = encoded_layers[-1] return sequence_output, attention_probs[-1] class BERT4RecPredictionHead(nn.Module): """ Predictionhead - MLM (Masked Language Model) """ def __init__(self, hidden_size: int, vocab_size: int): super().__init__() self.transform = nn.Linear(hidden_size, hidden_size) self.activation = nn.GELU() self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12) # mostendPredictionlayer (hidden_size -> vocab_size) self.decoder = nn.Linear(hidden_size, vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(vocab_size)) print(f"🎯 PredictionheadInitialize: {hidden_size} -> {vocab_size}") def forward(self, hidden_states): """ Predictionmask item """ hidden_states = self.transform(hidden_states) hidden_states = self.activation(hidden_states) hidden_states = self.LayerNorm(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states class BERT4RecForMaskedLM(nn.Module): """ Training BERT4RecModel (containsMLM) """ def __init__(self, config: Dict): super().__init__() self.bert = BERT4RecModel(config) self.mask_token_id = config.get('mask_token_id', 1) #[MASK] token ID print("📚 BERT4Rec MLMModelInitializeComplete") def forward(self, input_ids, masked_lm_labels=None): """ Forward propagation - MLMTraining """ sequence_output, attention_probs = self.bert(input_ids) # Prediction item (packagemask ) prediction_scores = self.bert.cls(sequence_output) outputs = (prediction_scores, attention_probs) # iflabels，Compute/CalculateMLMLoss if masked_lm_labels is not None: loss_fct = nn.CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct( prediction_scores.view(-1, self.bert.vocab_size), masked_lm_labels.view(-1) ) outputs = (masked_lm_loss,) + outputs return outputs def create_attention_mask(input_ids, pad_token_id=0): """ Createnotemask - Modelclosepadding """ attention_mask = (input_ids != pad_token_id).float() # Convert/Transformforcombineattention style [batch_size, 1, 1, seq_length] extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # willpaddingforlargenegativenumber (softmaxbackconnect0) extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 return extended_attention_mask def random_masking(input_ids, mask_token_id=1, mask_prob=0.15, pad_token_id=0): """ Randommask items - BERT4Rec centerTrainingstrategy """ batch_size, seq_length = input_ids.shape # Createlabels (mask trueTags) labels = input_ids.clone() # CreaterateMatrixfixedmask probability_matrix = torch.full(labels.shape, mask_prob) # mask padding padding_mask = (input_ids == pad_token_id) probability_matrix.masked_fill_(padding_mask, value=0.0) # Randomfixedmask masked_indices = torch.bernoulli(probability_matrix).bool() # needCompute/Calculateloss for-1 labels[~masked_indices] = -1 # 80% rateswitchfor[MASK] indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices input_ids[indices_replaced] = mask_token_id # 10% rateswitchforRandomtoken indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced random_words = torch.randint(2, input_ids.size(-1), labels.shape, dtype=torch.long) input_ids[indices_random] = random_words[indices_random] # 10%change return input_ids, labels def demo_bert4rec_structure(): """ demoBERT4Rec and """ print("🎯 BERT4Recdemo") print("=" * 60) # 1. ModelConfiguration config = { 'vocab_size': 1000, #Producttablelargesmall 'hidden_size': 64, #layerdegree 'num_hidden_layers': 2, #Transformerlayernumber 'num_attention_heads': 2, #noteheadnumber 'intermediate_size': 256, #FFNlayerlargesmall 'max_seq_length': 50, #mostlargedegree 'dropout_prob': 0.1, #Dropoutrate 'mask_token_id': 1 # [MASK] token ID } print(f"\n📋 1. ModelConfiguration:") for key, value in config.items(): print(f" {key}: {value}") # 2. InitializeModel print(f"\n🏗️ 2. ModelInitialize:") model = BERT4RecForMaskedLM(config) # 3. mock/simulateInputnumber print(f"\n📊 3. number:") batch_size = 2 seq_length = 10 # mock/simulateUserBehavior [item1, item2, item3, ...] input_ids = torch.randint(2, 100, (batch_size, seq_length)) print(f"begin: {input_ids}") # 4. mask masked_ids, labels = random_masking(input_ids, config['mask_token_id']) print(f"Maskback: {masked_ids}") print(f"Tags: {labels}") # 5. Forward propagation print(f"\n🔄 4. Forward propagation:") with torch.no_grad(): outputs = model(masked_ids, labels) loss, predictions, attention = outputs print(f"MLMLoss: {loss.item():.4f}") print(f" predictionshape: {predictions.shape}") #[batch_size, seq_length, vocab_size] print(f" noteshape: {attention.shape}") #[batch_size, num_heads, seq_length, seq_length] # 6. Modelminute print(f"\n🔍 5. Modelminute:") total_params = sum(p.numel() for p in model.parameters()) trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) print(f"totalParametersamount: {total_params:,}") print(f"TrainingParameters: {trainable_params:,}") # 7. note (simplified) print(f"\n👁️ 6. noteWeightsminute:") # getindividualkind、individualhead note sample_attention = attention[0, 0].numpy() # [seq_length, seq_length] print(f"noteMatrixshape: {sample_attention.shape}") print(f"individualclosedegreemosthigh : {np.argmax(sample_attention[0])}") print(f"averagenoteminutenumber: {np.mean(sample_attention):.4f}") print(f"\n✅ BERT4RecdemoComplete!") print(f"\n🎓 closepoint:") print(f"• Embeddinglayer: Item + Position Embedding") print(f"• Encoding: {config['num_hidden_layers']}layerTransformer Blocks") print(f"• note: {config['num_attention_heads']}headMulti-head attention") print(f"• Training: MLM，Randommask 15% items") print(f"• : ，frontbackcontext") return model if __name__ == "__main__": # Rundemo model = demo_bert4rec_structure() print(f"\n🎯 downsuggestion:") print(f"1. Multi-Head Attention Compute/Calculatethrough") print(f"2. MLM maskstrategy") print(f"3. with DeepFMcombine") print(f"4. optimize")