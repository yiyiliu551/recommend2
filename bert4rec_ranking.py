#!/usr/bin/env python3 """ BERT4RecFine rankingimplementation - based on forRecall CandidatesProductenterFine rankingminute """ import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from typing import List, Dict, Tuple import time class BERT4RecModel(nn.Module): """BERT4RecModel - based onTensorFlowimplementation """ def __init__( self, vocab_size: int = 10000, hidden_size: int = 256, num_hidden_layers: int = 2, num_attention_heads: int = 4, intermediate_size: int = 1024, max_position_embeddings: int = 200, dropout_prob: float = 0.1 ): super().__init__() self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers # Embeddings self.item_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size) self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12) self.dropout = nn.Dropout(dropout_prob) # Transformer Encoder Layers self.encoder_layers = nn.ModuleList([ TransformerLayer( hidden_size=hidden_size, num_attention_heads=num_attention_heads, intermediate_size=intermediate_size, dropout_prob=dropout_prob ) for _ in range(num_hidden_layers) ]) # Output projection (MLMTraining) self.output_bias = nn.Parameter(torch.zeros(vocab_size)) def forward(self, input_ids, attention_mask=None): """Forward propagation""" seq_length = input_ids.size(1) position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) # Embeddings items_embeddings = self.item_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) embeddings = items_embeddings + position_embeddings embeddings = self.layernorm(embeddings) embeddings = self.dropout(embeddings) # Transformer Encoder hidden_states = embeddings for layer in self.encoder_layers: hidden_states = layer(hidden_states, attention_mask) return hidden_states def get_item_embeddings(self): """GetProductEmbeddingMatrix，Candidatesminute""" return self.item_embeddings.weight class TransformerLayer(nn.Module): """individualTransformerlayer""" def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_prob): super().__init__() # Multi-Head Attention self.attention = MultiHeadAttention( hidden_size=hidden_size, num_attention_heads=num_attention_heads, dropout_prob=dropout_prob ) # Feed Forward self.intermediate = nn.Linear(hidden_size, intermediate_size) self.output_dense = nn.Linear(intermediate_size, hidden_size) # Layer Norm self.attention_layernorm = nn.LayerNorm(hidden_size, eps=1e-12) self.output_layernorm = nn.LayerNorm(hidden_size, eps=1e-12) # Dropout self.dropout = nn.Dropout(dropout_prob) def forward(self, hidden_states, attention_mask=None): # Self-Attention attention_output = self.attention(hidden_states, attention_mask) attention_output = self.dropout(attention_output) attention_output = self.attention_layernorm(attention_output + hidden_states) # Feed Forward intermediate_output = F.gelu(self.intermediate(attention_output)) layer_output = self.output_dense(intermediate_output) layer_output = self.dropout(layer_output) layer_output = self.output_layernorm(layer_output + attention_output) return layer_output class MultiHeadAttention(nn.Module): """multipleheadAttention mechanism""" def __init__(self, hidden_size, num_attention_heads, dropout_prob): super().__init__() self.num_attention_heads = num_attention_heads self.attention_head_size = int(hidden_size / num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(hidden_size, self.all_head_size) self.key = nn.Linear(hidden_size, self.all_head_size) self.value = nn.Linear(hidden_size, self.all_head_size) self.dense = nn.Linear(hidden_size, hidden_size) self.dropout = nn.Dropout(dropout_prob) def transpose_for_scores(self, x): new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None): query_layer = self.transpose_for_scores(self.query(hidden_states)) key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # Attention scores attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / np.sqrt(self.attention_head_size) if attention_mask is not None: attention_scores = attention_scores + attention_mask attention_probs = nn.Softmax(dim=-1)(attention_scores) attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_shape) output = self.dense(context_layer) return output class BERT4RecRanker: """BERT4RecFine ranking - forRecallCandidatesenterminuteSort""" def __init__(self, model: BERT4RecModel, device='cpu'): self.model = model.to(device) self.device = device self.model.eval() def score_candidates( self, user_sequence: List[int], candidates: List[int], method: str = 'next_item' ) -> Dict[int, float]: """ forCandidatesProductminute Args: user_sequence: UserHistory [item1, item2, ...] candidates: CandidatesProductList method: minutemethod - 'next_item': prediction - 'sequence_prob': compute Returns: ProductIDtominutenumber """ scores = {} if method == 'next_item': scores = self._score_next_item(user_sequence, candidates) elif method == 'sequence_prob': scores = self._score_sequence_prob(user_sequence, candidates) else: raise ValueError(f"Unknown scoring method: {method}") return scores def _score_next_item(self, user_sequence: List[int], candidates: List[int]) -> Dict[int, float]: """PredictionCandidatesfordownindividualProduct rate""" with torch.no_grad(): # Input input_ids = torch.tensor([user_sequence], dtype=torch.long).to(self.device) # Get table hidden_states = self.model(input_ids) # [1, seq_len, hidden_size] last_hidden = hidden_states[0, -1, :] #mostback Status/State # GetProductEmbeddingMatrix item_embeddings = self.model.get_item_embeddings() # [vocab_size, hidden_size] # Compute/CalculateCandidatesProduct minutenumber scores = {} for item_id in candidates: if item_id < self.model.vocab_size: item_emb = item_embeddings[item_id] # pointforminutenumber score = torch.dot(last_hidden, item_emb).item() scores[item_id] = score else: scores[item_id] = -float('inf') return scores def _score_sequence_prob(self, user_sequence: List[int], candidates: List[int]) -> Dict[int, float]: """Compute/CalculateaddinCandidatesback combineminutenumber""" scores = {} with torch.no_grad(): for item_id in candidates: # containsCandidates seq_with_item = user_sequence + [item_id] input_ids = torch.tensor([seq_with_item], dtype=torch.long).to(self.device) # GetList hidden_states = self.model(input_ids) # Compute/Calculate minutenumber # simplified：averageStatus/State L2numberforminutenumber seq_score = torch.norm(hidden_states.mean(dim=1)).item() scores[item_id] = seq_score return scores def rank( self, user_sequence: List[int], candidates: List[int], top_k: int = 100, method: str = 'next_item' ) -> List[Tuple[int, float]]: """ Fine rankingmainfunction Args: user_sequence: UserHistory candidates: Recall CandidatesProduct top_k: returnTop-KindividualProduct method: minutemethod Returns: Sortback (ProductID, minutenumber)List """ # forCandidatesminute scores = self.score_candidates(user_sequence, candidates, method) # SortreturnTop-K ranked_items = sorted(scores.items(), key=lambda x: x[1], reverse=True) return ranked_items[:top_k] class RecallService: """mock/simulateRecallservice""" def __init__(self, all_items: int = 10000): self.all_items = all_items def recall_candidates(self, user_id: int, num_candidates: int = 1000) -> List[int]: """mock/simulatemultipleRecall""" # Collaborative filteringRecall cf_items = np.random.choice(self.all_items, size=num_candidates//3, replace=False) # VectorRecall vector_items = np.random.choice(self.all_items, size=num_candidates//3, replace=False) # Popular/HotRecall popular_items = np.random.choice(100, size=num_candidates//3, replace=False) # Mergego candidates = list(set(cf_items) | set(vector_items) | set(popular_items)) return candidates[:num_candidates] def demo_ranking_pipeline(): """demo Fine ranking""" print("=" * 60) print("BERT4Rec Fine rankingsystemdemo") print("=" * 60) # 1. InitializeModel print("\n1. InitializeBERT4RecModel...") model = BERT4RecModel( vocab_size=10000, hidden_size=128, num_hidden_layers=2, num_attention_heads=4 ) ranker = BERT4RecRanker(model, device='cpu') recall_service = RecallService() # 2. mock/simulateUserBehavior user_sequence = [102, 205, 301, 456, 789, 234, 567, 890, 123, 456] print(f"\n2. UserHistory (most10individualProduct):") print(f" {user_sequence}") # 3. Recallsegment print("\n3. Recallsegment...") candidates = recall_service.recall_candidates(user_id=12345, num_candidates=1000) print(f"Recall {len(candidates)} individualCandidatesProduct") # 4. Fine rankingsegment print("\n4. BERT4Rec Fine rankingsegment...") start_time = time.time() # usenext_itemmethodminute ranked_items = ranker.rank( user_sequence=user_sequence, candidates=candidates, top_k=100, method='next_item' ) ranking_time = time.time() - start_time # 5. result print(f"\n5. Fine rankingresult (Top-10):") for i, (item_id, score) in enumerate(ranked_items[:10], 1): print(f"{i}. ProductID: {item_id:4d}, minutenumber: {score:.4f}") print(f"\n6. performance:") print(f"- CandidatesProductnumber: {len(candidates)}") print(f"- Fine rankinghour: {ranking_time*1000:.2f}ms") print(f"- averageindividualCandidates: {ranking_time*1000/len(candidates):.4f}ms") # 7. forratiominutemethod print("\n7. forratiominutemethod:") # method1: next_item sample_candidates = candidates[:50] scores_next = ranker.score_candidates(user_sequence, sample_candidates, 'next_item') # method2: sequence_prob scores_seq = ranker.score_candidates(user_sequence, sample_candidates, 'sequence_prob') print(f"next_itemmethod Top-3: {list(sorted(scores_next.items(), key=lambda x: x[1], reverse=True)[:3])}") print(f"sequence_probmethod Top-3: {list(sorted(scores_seq.items(), key=lambda x: x[1], reverse=True)[:3])}") def benchmark_performance(): """performanceTest""" print("\n" + "=" * 60) print("performanceTest") print("=" * 60) model = BERT4RecModel(vocab_size=10000, hidden_size=128) ranker = BERT4RecRanker(model) # Test test_configs = [ (10, 100), #10individualHistory，100individualCandidates (20, 500), #20individualHistory，500individualCandidates (50, 1000), #50individualHistory，1000individualCandidates (100, 2000), #100individualHistory，2000individualCandidates ] for seq_len, num_candidates in test_configs: user_sequence = list(range(1, seq_len + 1)) candidates = list(range(100, 100 + num_candidates)) start = time.time() ranked = ranker.rank(user_sequence, candidates, top_k=100) elapsed = time.time() - start print(f"\ndegree={seq_len}, Candidatesnumber={num_candidates}:") print(f"totalhour: {elapsed*1000:.2f}ms") print(f" QPS: {1/elapsed:.2f}") print(f"individualCandidates: {elapsed*1000/num_candidates:.4f}ms") if __name__ == "__main__": # demoFine ranking demo_ranking_pipeline() # performanceTest benchmark_performance() print("\n" + "=" * 60) print("total：BERT4RecFine rankingsystem") print("=" * 60) print(""" 1. Recallsegment：multipleRecallGetCandidates（secondlevel） 2. Fine rankingsegment：BERT4RecforCandidatesminute（10-100ms） 3. optimize： - Modelamount (FP16/INT8) - batchamountInference - Cache - Model close： - copydependencyclose - Attention mechanismfrontback - canselect minutestrategy """)